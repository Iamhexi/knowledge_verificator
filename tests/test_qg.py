"""Module with Question Generation module tests."""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import numpy as np
import pytest

from sentence_transformers import SentenceTransformer
from transformers import set_seed  # type: ignore[import-untyped]
from knowledge_verificator.qg import QuestionGeneration
from knowledge_verificator.utils.filesystem import create_text_file


class Metric(Enum):
    """List of metrics available."""

    COSINE_SIMILARITY = 0


@dataclass
class Result:
    """Dataclass representing a result of a test."""

    model_name: str
    metric: Metric
    data_points: np.ndarray


def generate_test_summary_in_csv(results: list[Result]) -> str:
    """
    Generate an experiment summary report in a CSV format.

    Args:
        results (list[Result]): List of results of experiments.

    Returns:
        str: Formatted CSV file with experiment results.
    """
    report = ''
    report += 'model_name,metric,average_score\n'
    for result in results:
        print('Data points: ', result.data_points)
        average_score = round(np.average(result.data_points), 3)
        report += f'{result.model_name},{result.metric.name},{average_score}\n'

    return report


@pytest.fixture
def qg():
    """
    Provide non-deterministically initialized instance of
    the `QuestionGeneration` class.
    """
    set_seed(0)
    question_generation = QuestionGeneration()
    return question_generation


@pytest.mark.code_quality
@pytest.mark.parametrize(
    'question,answer,context',
    (
        (
            'Where is the red apple located?',
            'Tree',
            'The red apple is on a tree.',
        ),
    ),
)
def test_basic_question_generation(
    question: str, answer: str, context: str, qg
):
    """Test if generating in very simple case works as expected."""
    output = qg.generate(answer=answer, context=context)
    expected = {
        'question': question,
        'answer': answer,
        'context': context,
    }

    assert output == expected


@pytest.mark.model_performance
def test_qg_performance_with_cosine_similarity(qg) -> None:
    """
    Test performance of question generation module using cosine similarity
    between reference question and generated by a model, provided that it
    has received context and an answer (if required).
    """
    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')

    test_data = [
        {
            'question': 'What color is the sky during the day?',
            'context': 'During the day, the sky appears blue.',
            'answer': 'blue',
        },
        {
            'question': 'What is the function of the frontend in software development?',
            'context': 'In software development, the terms frontend and backend refer to the distinct roles of the user interface (frontend) and the data management layer (backend) of an application. In a client-server architecture, the client typically represents the frontend, while the server represents the backend, even if some presentation tasks are handled by the server.',
            'answer': 'presentation layer',
        },
    ]

    metric = Metric.COSINE_SIMILARITY
    model_name = qg.trained_model_path.split('/')[1]
    data_points = np.zeros(shape=(len(test_data), 1))

    for i, test_item in enumerate(test_data):
        suggested_answer = test_item['answer']
        context = test_item['context']
        reference_question = test_item['question']

        generated_question = qg.generate(
            answer=suggested_answer, context=context
        )['question']

        sentences = [generated_question, reference_question]

        embeddings = model.encode(sentences)

        # Outputs tensor:
        # 1.0 abc
        # abc 1.0
        similarities = model.similarity(embeddings, embeddings)
        print('similarities:', similarities)

        data_point = float(similarities.tolist()[1][0])
        data_points[i] = data_point

    result = Result(
        metric=metric,
        data_points=data_points,
        model_name=model_name,
    )

    experimental_results = generate_test_summary_in_csv(results=[result])
    current_datetime = datetime.now().strftime('%H_%M_%S_%Y_%m_%d')
    create_text_file(
        path=f'tests/results/qg_{current_datetime}',
        content=experimental_results,
    )
